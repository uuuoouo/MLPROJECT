{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"vscode":{"interpreter":{"hash":"bb3398f4b21c7b026dd5874af3f954bf25f1e8ff81e25d82a94abcbbaacf760b"}},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7138603,"sourceType":"datasetVersion","datasetId":4119732},{"sourceId":7143958,"sourceType":"datasetVersion","datasetId":4123606}],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Hello and welcome to my ML project. \nI was initially going to do an app (option A), but I switched gears and chose a data exploration project (option C) in the end. This is the notebook for the NLP model I did. My goal was to have the model guess the card's colour based on it's name and card type. First we get the data.","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport torch\n\n\nall_cards_df = pd.read_csv('/kaggle/input/all-mtg-cards/all_mtg_cards.csv')\nselected_columns = all_cards_df[['name']]\nprint(selected_columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T16:48:47.084617Z","iopub.execute_input":"2024-12-12T16:48:47.084968Z","iopub.status.idle":"2024-12-12T16:48:54.510414Z","shell.execute_reply.started":"2024-12-12T16:48:47.084939Z","shell.execute_reply":"2024-12-12T16:48:54.509603Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"As you can see, there are many duplicate cards in this data set. My first step of cleaning is removing these redundant entries.","metadata":{}},{"cell_type":"code","source":"all_cards_no_dups = all_cards_df.drop_duplicates(subset=\"name\")\n\nprint(all_cards_no_dups[\"name\"].count())\nall_cards_no_dups[\"color_identity\"].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T16:48:54.511919Z","iopub.execute_input":"2024-12-12T16:48:54.512193Z","iopub.status.idle":"2024-12-12T16:48:54.541441Z","shell.execute_reply.started":"2024-12-12T16:48:54.512169Z","shell.execute_reply":"2024-12-12T16:48:54.540646Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The rows went from 76k to 26k. Because some of these card categories have so few numbers, I'm only going to have the model try to guess between monocolours, colorless, or multicolor. I'm going to create a new column 'overall_color' for this.","metadata":{}},{"cell_type":"code","source":"def str_to_list(cell):\n    cell = ''.join(c for c in cell if c not in \"'[]\")  \n    cell = cell.split(', ') \n    return cell\n\n# Define an overall card color category, including Colorless and Multi\ndef color_to_category(x):\n    try:\n        size = len(x)\n        if size == 0:\n            return \"Colorless\"\n        elif size == 1:\n            return x[0]\n        else:\n            return \"Multi\"\n    except:\n        return \"None\"\n\n\nall_cards_no_dups = all_cards_no_dups.copy()  # Create an explicit copy to avoid ambiguity\n\nall_cards_no_dups.loc[:, \"overall_color\"] = all_cards_no_dups[\"color_identity\"].apply(str)\nall_cards_no_dups.loc[:, \"overall_color\"] = all_cards_no_dups[\"overall_color\"].apply(lambda x: \"[]\" if x == \"nan\" else x)\nall_cards_no_dups.loc[:, \"overall_color\"] = all_cards_no_dups[\"overall_color\"].apply(eval)\nall_cards_no_dups.loc[:, \"overall_color\"] = all_cards_no_dups[\"overall_color\"].apply(color_to_category)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T16:48:54.542451Z","iopub.execute_input":"2024-12-12T16:48:54.542696Z","iopub.status.idle":"2024-12-12T16:48:54.715013Z","shell.execute_reply.started":"2024-12-12T16:48:54.542674Z","shell.execute_reply":"2024-12-12T16:48:54.714297Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define a mapping of shorthand to full names\ncolor_mapping = {\n    'B': 'Black',\n    'W': 'White',\n    'U': 'Blue',\n    'R': 'Red',\n    'G': 'Green',\n    'Multi': 'Multicolor'\n}\n\n# Replace the values in the 'overall_color' column using the mapping\nall_cards_no_dups['overall_color'] = all_cards_no_dups['overall_color'].replace(color_mapping)\n\n# Display the unique values in the updated column to verify\nprint(all_cards_no_dups['overall_color'].unique())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T16:48:54.716784Z","iopub.execute_input":"2024-12-12T16:48:54.717074Z","iopub.status.idle":"2024-12-12T16:48:54.732275Z","shell.execute_reply.started":"2024-12-12T16:48:54.717050Z","shell.execute_reply":"2024-12-12T16:48:54.731605Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Modeling","metadata":{}},{"cell_type":"markdown","source":"The color categories are looking nice. Now it's time to move on to setting up the model. I'm feeling hopeful as names and types should provide great insight into color. Angelic/holy language for white, zombies/undead for black, nature for green, goblins for red, equipment/artifact for colorless, etc.","metadata":{}},{"cell_type":"code","source":"card_color_subtypes = all_cards_no_dups.loc[:, [\"name\", \"type\", \"overall_color\"]]\ncard_color_subtypes = card_color_subtypes.explode(\"type\")\ncard_color_subtypes.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T16:48:54.733125Z","iopub.execute_input":"2024-12-12T16:48:54.733333Z","iopub.status.idle":"2024-12-12T16:48:54.757611Z","shell.execute_reply.started":"2024-12-12T16:48:54.733314Z","shell.execute_reply":"2024-12-12T16:48:54.756907Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"all_cards_no_dups['input_text'] = all_cards_no_dups['name'] + \" \" + all_cards_no_dups['type']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T16:48:54.758507Z","iopub.execute_input":"2024-12-12T16:48:54.758747Z","iopub.status.idle":"2024-12-12T16:48:54.769670Z","shell.execute_reply.started":"2024-12-12T16:48:54.758727Z","shell.execute_reply":"2024-12-12T16:48:54.768854Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Combining the type and name into one to make it more seamless. Regarding stopwords and capitalisation, I was assured by the sources that I looked up that preprocessing my data to remove those was largely unneccesary when using BERT, so I skipped that step of cleaning.","metadata":{}},{"cell_type":"markdown","source":"Splitting the data into test and training, also setting a random state to be able to track progress accurately.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(\n    all_cards_no_dups['input_text'], \n    all_cards_no_dups['overall_color'], \n    test_size=0.20, \n    random_state=41\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T16:48:54.770743Z","iopub.execute_input":"2024-12-12T16:48:54.771032Z","iopub.status.idle":"2024-12-12T16:48:55.298866Z","shell.execute_reply.started":"2024-12-12T16:48:54.771010Z","shell.execute_reply":"2024-12-12T16:48:55.297674Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The LabelEncoder is used to convert categorical labels into numeric format for both the training and validation datasets. The fit_transform method fits the encoder to the training labels and transforms them, while the transform method ensures the validation labels are encoded consistently using the same mapping.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nlabel_encoder = LabelEncoder()\ntrain_labels = label_encoder.fit_transform(train_labels)\nval_labels = label_encoder.transform(val_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T16:48:55.299964Z","iopub.execute_input":"2024-12-12T16:48:55.300204Z","iopub.status.idle":"2024-12-12T16:48:55.309583Z","shell.execute_reply.started":"2024-12-12T16:48:55.300184Z","shell.execute_reply":"2024-12-12T16:48:55.308539Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Bert tokenizer here to do my work for me.\n# It processes both training and validation texts with truncation, padding, and a maximum sequence length of 128 to ensure uniform input size\n\nfrom transformers import BertTokenizer\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\ntrain_encodings = tokenizer(list(train_texts), truncation=True, padding=True, max_length=128)\nval_encodings = tokenizer(list(val_texts), truncation=True, padding=True, max_length=128)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T17:29:47.222468Z","iopub.execute_input":"2024-12-12T17:29:47.222804Z","iopub.status.idle":"2024-12-12T17:29:52.151112Z","shell.execute_reply.started":"2024-12-12T17:29:47.222777Z","shell.execute_reply":"2024-12-12T17:29:52.150363Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# preparing training and validation sets\n\nclass CardDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\ntrain_dataset = CardDataset(train_encodings, train_labels)\nval_dataset = CardDataset(val_encodings, val_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T17:29:55.181820Z","iopub.execute_input":"2024-12-12T17:29:55.182661Z","iopub.status.idle":"2024-12-12T17:29:55.187877Z","shell.execute_reply.started":"2024-12-12T17:29:55.182631Z","shell.execute_reply":"2024-12-12T17:29:55.186962Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import BertForSequenceClassification\n\n# Loading the pre-trained BERT model with classification head\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_encoder.classes_))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T17:29:58.039976Z","iopub.execute_input":"2024-12-12T17:29:58.040365Z","iopub.status.idle":"2024-12-12T17:29:59.691132Z","shell.execute_reply.started":"2024-12-12T17:29:58.040320Z","shell.execute_reply":"2024-12-12T17:29:59.690392Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom torch.optim import AdamW\nfrom transformers import get_scheduler\n\n# Tried many batch sizes from 4 to 32\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32)\n\n# Set up device\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\n# Load pre-trained BERT model \nfrom transformers import BertForSequenceClassification\n\nnum_labels = len(set(train_labels)) \nmodel = BertForSequenceClassification.from_pretrained('bert-large-uncased', num_labels=num_labels)\n\nmodel.to(device)\n\n# Here I tried freezing some layers to reduce training time and reduce overfit which was a consistent problem\n\nfor param in model.base_model.parameters():\n    param.requires_grad = False\n\nfor param in model.base_model.encoder.layer[-6:].parameters():\n    param.requires_grad = True\n\nfor param in model.classifier.parameters():\n    param.requires_grad = True\n\n# Regularisation techniques \n# Dropout to, again, help with overfitting. Tried values from .2 to .4\nmodel.config.hidden_dropout_prob = 0.3\n\n# Weight decay: L2 regularisation to attempt to help with overfitting, rates from 1e-5 to 5e-6\noptimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=0.01)\n\n# Define learning rate scheduler\nnum_training_steps = len(train_loader) * 10\nlr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n\n# Early stopping to save time when the model dead ends.\nbest_val_loss = float('inf')\npatience = 3  \nepochs_no_improve = 0\n\n# Training loop with early stopping and scheduler\nfor epoch in range(1):  \n    model.train()  \n    total_train_loss = 0\n\n    for batch in train_loader:\n        inputs = {key: val.to(device) for key, val in batch.items() if key != 'labels'}\n        labels = batch['labels'].to(device)\n\n        optimizer.zero_grad()\n        outputs = model(**inputs, labels=labels)  \n        loss = outputs.loss\n        total_train_loss += loss.item()\n        loss.backward()\n        optimizer.step()\n        lr_scheduler.step()\n\n    avg_train_loss = total_train_loss / len(train_loader)\n    print(f\"Epoch {epoch + 1}, Training Loss: {avg_train_loss:.4f}\")\n\n    # Validation loop\n    model.eval()  \n    total_val_loss = 0\n    correct_predictions = 0\n\n    with torch.no_grad():\n        for batch in val_loader:\n            inputs = {key: val.to(device) for key, val in batch.items() if key != 'labels'}\n            labels = batch['labels'].to(device)\n\n            outputs = model(**inputs, labels=labels)  \n            total_val_loss += outputs.loss.item()\n\n            predictions = torch.argmax(outputs.logits, dim=-1)\n            correct_predictions += (predictions == labels).sum().item()\n\n    avg_val_loss = total_val_loss / len(val_loader)\n    val_accuracy = correct_predictions / len(val_dataset)\n\n    print(f\"Epoch {epoch + 1}, Validation Loss: {avg_val_loss:.4f}, Accuracy: {val_accuracy:.4f}\")\n\n    # Early stopping check\n    if avg_val_loss < best_val_loss:\n        best_val_loss = avg_val_loss\n        epochs_no_improve = 0\n    else:\n        epochs_no_improve += 1\n\n    if epochs_no_improve >= patience:\n        print(\"Early stopping triggered.\")\n        break\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T18:57:33.754807Z","iopub.execute_input":"2024-12-12T18:57:33.755621Z","iopub.status.idle":"2024-12-12T19:17:24.530969Z","shell.execute_reply.started":"2024-12-12T18:57:33.755589Z","shell.execute_reply":"2024-12-12T19:17:24.529895Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"1:\nEpoch 1, Training Loss: 1.1506\nEpoch 1, Validation Loss: 1.0580, Accuracy: 0.6295\nEpoch 2, Training Loss: 0.8626\nEpoch 2, Validation Loss: 1.0819, Accuracy: 0.6367\nEpoch 3, Training Loss: 0.6016\nEpoch 3, Validation Loss: 1.1341, Accuracy: 0.6330\n\nGoing for better stats. Adding dropout to .2 and changing learning rate to 2e-5\n\n2:\nEpoch 1, Training Loss: 1.2940\nEpoch 1, Validation Loss: 1.0733, Accuracy: 0.6245\nEpoch 2, Training Loss: 0.9318\nEpoch 2, Validation Loss: 1.0418, Accuracy: 0.6488\nEpoch 3, Training Loss: 0.7069\nEpoch 3, Validation Loss: 1.0732, Accuracy: 0.6374\n\nStill overfitting. Adjusting learning rate to 1e-5 with weight decay .01, increasing batchsize from 16 to 20, increasing dropout to .3. Will consider trying layer freezing and dataset balancing next. Increasing epochs to 5\n\n3:\nEpoch 1, Training Loss: 0.4347\nEpoch 1, Validation Loss: 1.1961, Accuracy: 0.6505\nEpoch 2, Training Loss: 0.3103\nEpoch 2, Validation Loss: 1.3321, Accuracy: 0.6388\nEpoch 3, Training Loss: 0.2293\nEpoch 3, Validation Loss: 1.4711, Accuracy: 0.6361\n\nStopped after 3rd epoch as the losses were too much to bear. Surprsingly first epoch is my best yet though lol. I'm increasing the test data size to .25 from .2. Dropout is going to .4, learning to 5-e6. I'm also trying layer freezing now. Data was already balanced just fine, so I'm not worried about that.\n\n4:\nEpoch 1, Training Loss: 1.7682\nEpoch 1, Validation Loss: 1.5257, Accuracy: 0.4651\nEpoch 2, Training Loss: 1.4664\nEpoch 2, Validation Loss: 1.3411, Accuracy: 0.5303\nEpoch 3, Training Loss: 1.3275\nEpoch 3, Validation Loss: 1.2543, Accuracy: 0.5584\nEpoch 4, Training Loss: 1.2560\nEpoch 4, Validation Loss: 1.2066, Accuracy: 0.5787\nEpoch 5, Training Loss: 1.2064\nEpoch 5, Validation Loss: 1.1840, Accuracy: 0.5847\n\nInteresting development. At least both validations are improving now. Increasing training rate 2e-5, reducing dropout to .3, freezing less layers (4 to 6)\n\nlatest attempt made it up to .63 but the numbers didnt save. trying some changes\n\nEpoch 1, Training Loss: 1.4009\nEpoch 1, Validation Loss: 1.1310, Accuracy: 0.6100\nEpoch 2, Training Loss: 1.0546\nEpoch 2, Validation Loss: 1.0551, Accuracy: 0.6302\nEpoch 3, Training Loss: 0.8790\nEpoch 3, Validation Loss: 1.0580, Accuracy: 0.6399\nEpoch 4, Training Loss: 0.7424\nEpoch 4, Validation Loss: 1.0737, Accuracy: 0.6432\nEpoch 5, Training Loss: 0.6216\nEpoch 5, Validation Loss: 1.1312, Accuracy: 0.6455\nEpoch 6, Training Loss: 0.5326\nEpoch 6, Validation Loss: 1.1673, Accuracy: 0.6466\nEpoch 7, Training Loss: 0.4555\nEpoch 7, Validation Loss: 1.2099, Accuracy: 0.6387\nEpoch 8, Training Loss: 0.3984\nEpoch 8, Validation Loss: 1.2552, Accuracy: 0.6404\nEpoch 9, Training Loss: 0.3577\nEpoch 9, Validation Loss: 1.2853, Accuracy: 0.6387\n\nI stopped documenting after here because I couldn't reliably get above .64 in many consecutive tries.","metadata":{}},{"cell_type":"markdown","source":"# Visualisations","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Data for losses\ntraining_losses = [1.5665, 1.2734, 1.1782, 1.1140, 1.0686, 1.0222, 0.9875, 0.9567, 0.9262, 0.9183]\nvalidation_losses = [1.2992, 1.2034, 1.1500, 1.1222, 1.1128, 1.1074, 1.0960, 1.0888, 1.0897, 1.0881]\nepochs = list(range(1, 11))\n\n# Plot for training and validation losses\nplt.figure(figsize=(10, 5))\nplt.plot(epochs, training_losses, label='Training Loss', marker='o')\nplt.plot(epochs, validation_losses, label='Validation Loss', marker='o')\nplt.title('Training and Validation Loss over Epochs')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.grid(True)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T19:34:58.298023Z","iopub.execute_input":"2024-12-12T19:34:58.298363Z","iopub.status.idle":"2024-12-12T19:34:58.557670Z","shell.execute_reply.started":"2024-12-12T19:34:58.298338Z","shell.execute_reply":"2024-12-12T19:34:58.556903Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Data for accuracy\naccuracies = [0.5432, 0.5769, 0.5912, 0.6049, 0.6145, 0.6195, 0.6236, 0.6253, 0.6278, 0.6278]\n\n# Plot for validation accuracy\nplt.figure(figsize=(10, 5))\nplt.plot(epochs, accuracies, label='Validation Accuracy', marker='o', color='green')\nplt.title('Validation Accuracy over Epochs')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.grid(True)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T19:35:00.745743Z","iopub.execute_input":"2024-12-12T19:35:00.746394Z","iopub.status.idle":"2024-12-12T19:35:00.915874Z","shell.execute_reply.started":"2024-12-12T19:35:00.746362Z","shell.execute_reply":"2024-12-12T19:35:00.915135Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Here's a couple graphs showing the loss and accuracy against epochs. This isn't from my best result, but these ones made for a nice graph, so I used this set of training haha.","metadata":{}},{"cell_type":"markdown","source":"# Confusion Matrix","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\n\n# Initialize lists to store true and predicted labels\ny_true = []\ny_pred = []\n\n# Validation loop \nmodel.eval()\nwith torch.no_grad():\n    for batch in val_loader:\n        inputs = {key: val.to(device) for key, val in batch.items() if key != 'labels'}\n        labels = batch['labels'].to(device)\n\n        outputs = model(**inputs, labels=labels)\n        predictions = torch.argmax(outputs.logits, dim=-1)\n\n        # Append true and predicted labels\n        y_true.extend(labels.cpu().numpy())\n        y_pred.extend(predictions.cpu().numpy())\n\n# Convert lists to numpy arrays \ny_true = np.array(y_true)\ny_pred = np.array(y_pred)\n\n# Define the category mapping\ncategory_mapping = ['Black', 'Blue', 'Colorless', 'Green', 'Multicolor', 'Red', 'White']\n\n# Compute the confusion matrix\ncm = confusion_matrix(y_true, y_pred)\n\n# Plot the confusion matrix using Seaborn with category labels\nplt.figure(figsize=(10, 8))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n            xticklabels=category_mapping, yticklabels=category_mapping)\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.title('Confusion Matrix')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T18:09:18.951728Z","iopub.execute_input":"2024-12-12T18:09:18.952084Z","iopub.status.idle":"2024-12-12T18:09:36.255189Z","shell.execute_reply.started":"2024-12-12T18:09:18.952056Z","shell.execute_reply":"2024-12-12T18:09:36.254242Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Here's a confusion matrix to track how the model did on each different area. It makes sense that multicolor was the most difficult to place correctly. It was consistently the worst of all the categories except for one stand out: Predicting white for blue slightly edged it out. This makes some amount of sense because in the game of MtG, white and blue share some thematic gameplay similarities.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import precision_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import f1_score\n\naccuracy = accuracy_score(y_true, y_pred)\nprint(f'Accuracy: {accuracy:.4f}')\n\nprecision = precision_score(y_true, y_pred, average='weighted')\nprint(f'Precision: {precision:.4f}')\n\nrecall = recall_score(y_true, y_pred, average='weighted')\nprint(f'Recall: {recall:.4f}')\n\nrmse = np.sqrt(mean_squared_error(y_true, y_pred))\nprint(f'RMSE: {rmse:.4f}')\n\nf1 = f1_score(y_true, y_pred, average='weighted')\nprint(f'F1 Score: {f1:.4f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T20:05:36.655398Z","iopub.execute_input":"2024-12-12T20:05:36.656162Z","iopub.status.idle":"2024-12-12T20:05:36.674548Z","shell.execute_reply.started":"2024-12-12T20:05:36.656132Z","shell.execute_reply":"2024-12-12T20:05:36.673495Z"}},"outputs":[],"execution_count":null}]}